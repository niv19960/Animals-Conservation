{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "l={}\n",
    "u=list()\n",
    "url= \"https://www.proxynova.com/proxy-server-list/country-ISO 3166-2:IL/\"\n",
    "respo = requests.get(url).text\n",
    "soup = BeautifulSoup(respo, \"html.parser\")\n",
    "allproxy = soup.find_all(\"tr\")\n",
    "for proxy in allproxy:\n",
    "    foo = proxy.find_all(\"td\")\n",
    "try: \n",
    "    l[\"ip\"]=foo[0].text.replace(\"\\n\",\"\").replace(\"document.write(\",\"\").replace(\")\",\"\").replace(\"\\'\",\"\").replace(\";\",\"\")\n",
    "except:\n",
    "    l[\"ip\"]=None\n",
    "try:\n",
    "    l[\"port\"]=foo[1].text.replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "except:\n",
    "    l[\"port\"]=None\n",
    "try:\n",
    "    l[\"country\"]=foo[5].text.replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "except:\n",
    "    l[\"country\"]=None\n",
    "if(l[\"port\"] is not None):\n",
    "    u.append(l)\n",
    "l={}\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract animals data from http://animalia.bio/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this part we convert the main html pages into BeautifulSoup format.\n",
    "We took three animals' classes: Mammals, Reptiles and Birds.\n",
    "\n",
    "Each class is located in different page in the main site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mammals_url = \"http://animalia.bio/mammals\"\n",
    "Reptiles_url = \"http://animalia.bio/reptiles\"\n",
    "Birds_url = \"http://animalia.bio/birds\"\n",
    "\n",
    "response_mammals = requests.get(Mammals_url)\n",
    "response_reptiles = requests.get(Reptiles_url)\n",
    "response_birds = requests.get(Birds_url)\n",
    "\n",
    "Mammals_soup = BeautifulSoup(response_mammals.content, \"html.parser\")\n",
    "Reptiles_soup = BeautifulSoup(response_reptiles.content, \"html.parser\")\n",
    "Birds_soup = BeautifulSoup(response_birds.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function gets a class page and return all animals' links which located at this specific page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_url(animal_soup):\n",
    "    tag_animals = animal_soup.find(\"div\", attrs = {\"class\":\"animals\"})\n",
    "    animals_links = [i['href'] for i in tag_animals.findAll(\"a\")]\n",
    "    return animals_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function gets current page and return the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_page(current_page): \n",
    "    tag_next_pages = current_page.find(\"div\", attrs = {\"class\":\"paginator\"})\n",
    "    pages = [i['href'] for i in tag_next_pages.findAll(\"a\")]\n",
    "    return pages[-1]                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function gets all three classes (Mammals, reptiles an Bird at BeautifulSoup format), and return a list which include all animals pages links together.\n",
    "```\n",
    " Number of pages at Mammals' class: 22\n",
    " Number of pages at Reptiles' class: 6\n",
    " Number of pages at Birds' class: 12\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_animals(Mammals_soup, Reptiles_soup, Birds_soup):\n",
    "    all_animals = []    # this list will contain all individuals' animals' pages\n",
    "    \n",
    "    \"\"\" For Mammals \"\"\"\n",
    "    \n",
    "    all_animals = all_animals + list_of_url(Mammals_soup)    # Extract the animals from the fisrt Mammals page  \n",
    "    next_Mammals_soup = Mammals_soup    # Save the first page in new variable\n",
    "    for i in range(1, 22):    # According to the number of pages for each class\n",
    "        new_url = get_next_page(next_Mammals_soup)\n",
    "        response_new_url = requests.get(new_url)\n",
    "        next_Mammals_soup = BeautifulSoup(response_new_url.content, \"html.parser\")    # Convert to BeautifulSoup format\n",
    "        all_animals = all_animals + list_of_url(next_Mammals_soup)    # Extract the animals from the current Mammals page\n",
    "        \n",
    "    \"\"\" For Reptiles \"\"\"\n",
    "        \n",
    "    all_animals = all_animals + list_of_url(Reptiles_soup)\n",
    "    next_Reptiles_soup = Reptiles_soup\n",
    "    for i in range(1, 6):\n",
    "        new_url = get_next_page(next_Reptiles_soup)\n",
    "        response_new_url = requests.get(new_url)\n",
    "        next_Reptiles_soup = BeautifulSoup(response_new_url.content, \"html.parser\")\n",
    "        all_animals = all_animals + list_of_url(next_Reptiles_soup)\n",
    "    \n",
    "    \"\"\" For Birds \"\"\"\n",
    "    \n",
    "    all_animals = all_animals + list_of_url(Birds_soup)\n",
    "    next_Birds_soup = Birds_soup\n",
    "    for i in range(1, 12):\n",
    "        new_url = get_next_page(next_Birds_soup)\n",
    "        response_new_url = requests.get(new_url)\n",
    "        next_Birds_soup = BeautifulSoup(response_new_url.content, \"html.parser\")\n",
    "        all_animals = all_animals + list_of_url(next_Birds_soup)\n",
    "        \n",
    "    return all_animals    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_all_animals = extract_all_animals(Mammals_soup, Reptiles_soup, Birds_soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1597"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links_all_animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('links_animals.txt', 'w') as f:\n",
    "    for link in links_all_animals:\n",
    "        f.write(link + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
